import os
import numpy as np
from typing import List, Optional, Any
from sklearn.metrics.pairwise import cosine_similarity
from loader import Chunk
import logging

logger = logging.getLogger(__name__)

# Default system prompt file path (can be overridden via environment variable)
SYSTEM_PROMPT_PATH = os.getenv("SYSTEM_PROMPT_PATH", "/app/system_prompt.txt")


def load_system_prompt() -> str:
    """Load custom system prompt from file if it exists."""
    base_prompt = "You are a helpful assistant. Use the provided context to answer the user's question. If the answer is not in the context, say you don't know."
    
    # Try multiple possible locations
    possible_paths = [
        SYSTEM_PROMPT_PATH,
        "system_prompt.txt",
        os.path.join(os.path.dirname(__file__), "..", "system_prompt.txt"),
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    custom_prompt = f.read().strip()
                if custom_prompt:
                    logger.info(f"Loaded system prompt from {path}")
                    return f"{base_prompt}\n\n{custom_prompt}"
            except Exception as e:
                logger.warning(f"Failed to read system prompt from {path}: {e}")
    
    return base_prompt


class ChatEngine:
    def __init__(self, chunks: List[Chunk]):
        self.chunks = chunks
        if chunks:
            # Pad embeddings if they have inconsistent lengths (shouldn't happen in valid index)
            # But let's check first dimension
            dim = len(chunks[0].embedding)
            valid_chunks = [c for c in chunks if len(c.embedding) == dim]
            if len(valid_chunks) != len(chunks):
                logger.warning(f"Filtered out {len(chunks) - len(valid_chunks)} chunks with inconsistent embedding dimensions.")

            self.chunks = valid_chunks

            # Create embeddings matrix
            try:
                self.embeddings = np.array([c.embedding for c in self.chunks])

                # Verify shape is 2D
                if len(self.embeddings.shape) != 2:
                    logger.error(f"Embeddings matrix has incorrect shape: {self.embeddings.shape}. Expected 2D array.")
                    # Try to force 2D if it's 1D (e.g. array of objects)
                    # But if we filtered correctly, it should be fine.
                    # If it fails here, we should probably disable search.
                    self.embeddings = None
            except Exception as e:
                logger.error(f"Failed to create embeddings matrix: {e}")
                self.embeddings = None
        else:
            self.embeddings = None

    def search(self, query_embedding: List[float], k: int = 3) -> List[Chunk]:
        if self.embeddings is None or len(self.embeddings) == 0:
            logger.warning("Search called but embeddings are empty or invalid.")
            return []

        # Double check shape before accessing index 1
        if len(self.embeddings.shape) < 2:
             logger.error(f"Embeddings shape {self.embeddings.shape} is not 2D. Cannot perform search.")
             return []

        # Ensure query embedding matches dimension
        if len(query_embedding) != self.embeddings.shape[1]:
            logger.error(f"Query embedding dimension {len(query_embedding)} does not match index dimension {self.embeddings.shape[1]}")
            return []

        query_embedding = np.array(query_embedding).reshape(1, -1)
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]

        # Get top k indices
        # If fewer than k chunks, take all
        k = min(k, len(self.chunks))
        top_k_indices = similarities.argsort()[-k:][::-1]

        return [self.chunks[i] for i in top_k_indices]

    def generate_response(self, query: str, context_chunks: List[Chunk], provider: str, api_key: str, **kwargs) -> str:
        context_text = "\n\n".join([f"Source: {c.source}\n{c.content}" for c in context_chunks])

        system_prompt = load_system_prompt()
        user_prompt = f"Context:\n{context_text}\n\nQuestion: {query}"

        if provider == "OpenAI":
            try:
                from openai import OpenAI
                client = OpenAI(api_key=api_key)
                response = client.chat.completions.create(
                    model=kwargs.get("model", "gpt-3.5-turbo"),
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                return response.choices[0].message.content
            except Exception as e:
                return f"Error calling OpenAI: {e}"

        elif provider == "Azure OpenAI":
            try:
                from openai import AzureOpenAI
                client = AzureOpenAI(
                    api_key=api_key,
                    api_version=kwargs.get("api_version", "2023-05-15"),
                    azure_endpoint=kwargs.get("azure_endpoint")
                )
                response = client.chat.completions.create(
                    model=kwargs.get("model"), # In Azure this is the deployment name
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                )
                return response.choices[0].message.content
            except Exception as e:
                return f"Error calling Azure OpenAI: {e}"

        elif provider == "Anthropic":
            try:
                import anthropic
                client = anthropic.Anthropic(api_key=api_key)
                response = client.messages.create(
                    model=kwargs.get("model", "claude-3-opus-20240229"),
                    max_tokens=1024,
                    messages=[
                        {"role": "user", "content": f"{system_prompt}\n\n{user_prompt}"}
                    ]
                )
                return response.content[0].text
            except Exception as e:
                return f"Error calling Anthropic: {e}"

        elif provider == "Mock":
            return f"Mock response. Context size: {len(context_text)} chars. Query: {query}"

        else:
            return "Provider not supported."

    def get_embedding(self, text: str, provider: str, api_key: str, **kwargs) -> List[float]:
        if provider == "OpenAI":
             try:
                 from openai import OpenAI
                 client = OpenAI(api_key=api_key)
                 response = client.embeddings.create(
                     input=text,
                     model=kwargs.get("embedding_model", "text-embedding-ada-002")
                 )
                 return response.data[0].embedding
             except Exception as e:
                 logger.error(f"OpenAI embedding error: {e}")
                 raise e

        elif provider == "Azure OpenAI":
            try:
                from openai import AzureOpenAI
                client = AzureOpenAI(
                    api_key=api_key,
                    api_version=kwargs.get("api_version", "2023-05-15"),
                    azure_endpoint=kwargs.get("azure_endpoint")
                )
                response = client.embeddings.create(
                    input=text,
                    model=kwargs.get("embedding_model") # In Azure this is the deployment name
                )
                return response.data[0].embedding
            except Exception as e:
                logger.error(f"Azure OpenAI embedding error: {e}")
                raise e

        elif provider == "Mock":
            # Return a random vector of appropriate size for testing
            # We need to know the target dimension.
            # If we have loaded chunks, we can use their dimension.
            if self.embeddings is not None and len(self.embeddings.shape) > 1 and self.embeddings.shape[1] > 0:
                dim = self.embeddings.shape[1]
                return list(np.random.rand(dim))
            return [0.1, 0.2, 0.3]

        else:
            # For other providers, we might need specific implementations.
            # Assuming OpenAI compatible for now or failing.
            raise ValueError(f"Embedding provider {provider} not implemented")
